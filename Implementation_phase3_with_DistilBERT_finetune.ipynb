  
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickym99/ljmu/blob/main/Implementation_phase3_with_DistilBERT_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0m36MaaSfwo",
        "outputId": "4a8fe17f-8c7f-4eca-8427-6bc21c4dac14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  2 10:32:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N9Nhq4gpEwG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import spacy\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, ClassLabel\n",
        "from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "6909c15a078c4c3a8e3f3542ebb804ab",
            "3f3e3a834b9245e4a334765b17674164",
            "75af5cc3bf9948ef8430e7dcab8a7436",
            "a08c6a44162b4452ab0101c74775c382",
            "4e76baa8fcf2403f96deeb7e5f1f7de0",
            "65bcd33551e142f2875905359091bfa2",
            "54c5177023224d8b8c65081803476d3e",
            "5e800fee79974ccfb527f4799323f883",
            "5b69776a3cda42bf99c52ebca5820caa",
            "e7970f69a13d4a44a9f0f568b0d5c542",
            "df179424f8ab4c01ad41a196b3f8a580",
            "6f343ea6c8554f998fc87d4aff00fd94",
            "571b7d5cd036458799e035b6a54fc137",
            "8bde35b647f042e2b6f2d526ad84a5a4",
            "3f200ce61905475584cf4988a56805b9",
            "800e3c48f5994c10949e06e9c4126bdb",
            "d94d701b530f4929b45838ab2f88406c",
            "f9b7f6161d37411e8a54531453a86096",
            "171fdd2475ad46188521dc0faf7d78ae",
            "fdea7a2d63714d29b92261747522ccb0"
          ]
        },
        "id": "XYcfOAMamnQg",
        "outputId": "7ac01a9d-3a30-4ada-96ea-dbe7a0ca2e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6909c15a078c4c3a8e3f3542ebb804ab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "wXY_tuzzgqsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    logger.error(\"SpaCy model 'en_core_web_sm' not found. Install it with: python -m spacy download en_core_web_sm\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "RXVqAIGTpJeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmfpRXZqpMcm",
        "outputId": "39d50bb5-a1f8-4b26-c406-9192ecdb611c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a batch size for processing - Reduced batch size\n",
        "batch_size = 8  # You can adjust this value based on your GPU memory\n",
        "print(f\"Using batch size: {batch_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT_vQ3wWpQUh",
        "outputId": "2628b6bd-28d5-49bf-8d8b-ae1256471c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using batch size: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def create_ticker_map_csv(output_path=\"/content/company_tickers.csv\"):\n",
        "    sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
        "    tickers = sp500['Symbol'].tolist()\n",
        "    data = []\n",
        "    for ticker in tickers:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        info = stock.info\n",
        "        company_name = info.get(\"longName\", ticker)\n",
        "        canonical_name = info.get(\"shortName\", company_name.split()[0])\n",
        "        data.append({\"Company_Name\": company_name, \"Ticker\": ticker, \"Canonical_Name\": canonical_name})\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "bUuwBaVkpWar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load entity map from CSV\n",
        "def load_entity_map(csv_path=\"/content/company_tickers.csv\"):\n",
        "    \"\"\"Load company-ticker mappings from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    entity_map = {}\n",
        "    for _, row in df.iterrows():\n",
        "        # Store both Company Name and Ticker mapping to Canonical Name\n",
        "        entity_map[str(row['Company_Name'])] = str(row['Canonical_Name'])\n",
        "        entity_map[str(row['Ticker'])] = str(row['Canonical_Name'])\n",
        "    return entity_map"
      ],
      "metadata": {
        "id": "4erPjdLopZmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing with SpaCy NER\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess financial text using SpaCy NER.\"\"\"\n",
        "    try:\n",
        "        text = text[:10000]  # Limit to 10,000 characters\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "        cleaned_text = soup.get_text()\n",
        "        cleaned_text = re.sub(r'[^\\w\\s\\$\\%\\.\\,]', '', cleaned_text)\n",
        "        sentences = nltk.sent_tokenize(cleaned_text)\n",
        "        normalized_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if len(sentence) > 1000:\n",
        "                continue\n",
        "            normalized_sentence = sentence\n",
        "            for ticker, canonical in load_entity_map(csv_path=\"/content/company_tickers.csv\").items():\n",
        "                pattern = r'\\b' + re.escape(ticker) + r'\\b'\n",
        "                normalized_sentence = re.sub(pattern, canonical, normalized_sentence, flags=re.IGNORECASE)\n",
        "            doc = nlp(normalized_sentence)\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ == \"ORG\":\n",
        "                    canonical_name = ent.text.split()[0]\n",
        "                    normalized_sentence = normalized_sentence.replace(ent.text, canonical_name)\n",
        "            normalized_sentences.append(normalized_sentence)\n",
        "        return normalized_sentences if normalized_sentences else [text[:512]]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error preprocessing text: {e}\")\n",
        "        return [text[:512]]"
      ],
      "metadata": {
        "id": "IX6-FWb0peG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Named Entity Recognition\n",
        "def extract_entities(sentences):\n",
        "    \"\"\"Extract entities using FinBERT.\"\"\"\n",
        "    try:\n",
        "        ner_pipeline = pipeline(\n",
        "            \"ner\",\n",
        "            model=\"ProsusAI/finbert\",\n",
        "            tokenizer=AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", max_length=512, truncation=True),\n",
        "            aggregation_strategy=\"simple\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        entities = []\n",
        "        for sentence in sentences:\n",
        "            ner_results = ner_pipeline(sentence)\n",
        "            sentence_entities = [{\"text\": entity[\"word\"], \"entity\": entity[\"entity_group\"], \"score\": entity[\"score\"]} for entity in ner_results]\n",
        "            entities.append({\"sentence\": sentence, \"entities\": sentence_entities})\n",
        "        return entities\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in NER: {e}\")\n",
        "        return [{\"sentence\": s, \"entities\": []} for s in sentences]"
      ],
      "metadata": {
        "id": "Ed_xzOKkpfNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Relevance Classification using FinBERT\n",
        "def classify_relevance(sentences):\n",
        "    \"\"\"Classify sentences as financially relevant.\"\"\"\n",
        "    try:\n",
        "        classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"ProsusAI/finbert\",\n",
        "            tokenizer=AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", max_length=512, truncation=True),\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        relevance_results = []\n",
        "        for sentence in sentences:\n",
        "            result = classifier(sentence, max_length=512, truncation=True)\n",
        "            is_relevant = result[0][\"label\"] == \"positive\" and result[0][\"score\"] > 0.3  # Lowered threshold\n",
        "            relevance_results.append({\"sentence\": sentence, \"is_relevant\": is_relevant, \"score\": result[0][\"score\"]})\n",
        "        return relevance_results\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in relevance classification: {e}\")\n",
        "        return [{\"sentence\": s, \"is_relevant\": True, \"score\": 0.0} for s in sentences]"
      ],
      "metadata": {
        "id": "tT5D9VH5piDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Sentiment Analysis\n",
        "def analyze_sentiment(sentences, model_path=\"distilbert-base-uncased\"):\n",
        "    \"\"\"Classify sentiment using DistilBERT.\"\"\"\n",
        "    try:\n",
        "        sentiment_pipeline = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=model_path,\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model_path, max_length=512, truncation=True),\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        sentiment_results = []\n",
        "        for sentence in sentences:\n",
        "            result = sentiment_pipeline(sentence, max_length=512, truncation=True)\n",
        "            predicted_label = result[0][\"label\"].lower()\n",
        "            binary_label = \"positive\" if predicted_label in [\"positive\", \"LABEL_1\"] else \"negative\"\n",
        "            sentiment_results.append({\n",
        "                \"sentence\": sentence,\n",
        "                \"sentiment\": binary_label,\n",
        "                \"score\": result[0][\"score\"]\n",
        "            })\n",
        "        return sentiment_results\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in sentiment analysis with {model_path}: {e}\")\n",
        "        return [{\"sentence\": s, \"sentiment\": \"negative\", \"score\": 0.0} for s in sentences]"
      ],
      "metadata": {
        "id": "UiAmFoYNpkwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load Combined_News_DJIA.csv\n",
        "def load_djia_dataset(file_path=None, max_rows=None):\n",
        "    \"\"\"Load Combined_News_DJIA dataset from URL or local file.\"\"\"\n",
        "    try:\n",
        "        if file_path:\n",
        "            df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
        "        else:\n",
        "            url = \"https://raw.githubusercontent.com/niharikabalachandra/Stock-Market-Prediction-Using-Natural-Language-Processing/master/Combined_News_DJIA.csv\"\n",
        "            df = pd.read_csv(url, encoding='utf-8', low_memory=False)\n",
        "\n",
        "        # Decode byte strings in Top1 to Top25 columns\n",
        "        for col in [f'Top{i}' for i in range(1, 26)]:\n",
        "            df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
        "\n",
        "        logger.info(f\"CSV loaded: {df.shape}, Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        df['text'] = df[[f'Top{i}' for i in range(1, 26)]].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "        df = df.dropna(subset=['text', 'Label'])\n",
        "        df['text'] = df['text'].astype(str)\n",
        "\n",
        "        if max_rows is not None:\n",
        "            df = df.sample(n=min(max_rows, len(df)), random_state=42)\n",
        "            logger.info(f\"Subsampled to {len(df)} rows\")\n",
        "\n",
        "        df['label'] = df['Label'].astype(int)  # 0 = negative, 1 = positive\n",
        "        dataset = Dataset.from_pandas(df[['text', 'label']])\n",
        "        dataset = dataset.cast_column(\"label\", ClassLabel(names=[\"negative\", \"positive\"]))\n",
        "\n",
        "        if len(dataset) < 2:\n",
        "            raise ValueError(\"Dataset is too small for train-test split\")\n",
        "\n",
        "        # spliting dataset into train-test\n",
        "        try:\n",
        "            dataset = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
        "        except ValueError as e:\n",
        "            logger.warning(f\"Stratified split failed: {e}. Using non-stratified split.\")\n",
        "            dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "        logger.info(f\"Dataset prepared: {len(dataset['train'])} train, {len(dataset['test'])} test\")\n",
        "        logger.info(f\"Train label distribution: {pd.Series(dataset['train']['label']).value_counts().to_dict()}\")\n",
        "        logger.info(f\"Test label distribution: {pd.Series(dataset['test']['label']).value_counts().to_dict()}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading DJIA CSV: {e}\")\n",
        "        sample_data = {\n",
        "            \"text\": [\"Apple reported strong earnings.\", \"Sales declined sharply.\"],\n",
        "            \"label\": [1, 0]\n",
        "        }\n",
        "        df_sample = pd.DataFrame(sample_data)\n",
        "        dataset = Dataset.from_pandas(df_sample)\n",
        "        dataset = dataset.cast_column(\"label\", ClassLabel(names=[\"negative\", \"positive\"]))\n",
        "        dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "        logger.info(\"Using sample data as fallback.\")\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "uSQ8SXGCSMcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Fine-Tune DistilBERT\n",
        "def fine_tune_distilbert(file_path=None, output_dir=\"./distilbert-finetuned-djia\", max_rows=None):\n",
        "    \"\"\"Fine-tune DistilBERT on Combined_News_DJIA dataset.\"\"\"\n",
        "    try:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"Using device: {device}\")\n",
        "\n",
        "        dataset = load_djia_dataset(file_path, max_rows)\n",
        "        if dataset is None or len(dataset[\"train\"]) == 0:\n",
        "            raise ValueError(\"Failed to load dataset\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"distilbert-base-uncased\",\n",
        "            num_labels=2,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model.to(device)\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "        tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "        tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "        labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
        "        unique_labels = np.unique(labels)\n",
        "        expected_labels = np.array([0, 1])\n",
        "        if not np.array_equal(np.sort(unique_labels), expected_labels):\n",
        "            logger.warning(f\"Expected labels {expected_labels}, found {unique_labels}. Adjusting class weights.\")\n",
        "            class_weights = np.zeros(2)\n",
        "            temp_weights = compute_class_weight(\"balanced\", classes=np.array(unique_labels), y=labels)\n",
        "            for i, label in enumerate(unique_labels):\n",
        "                class_weights[label] = temp_weights[i]\n",
        "        else:\n",
        "            class_weights = compute_class_weight(\"balanced\", classes=np.array(unique_labels), y=labels)\n",
        "\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "        logger.info(f\"Class weights: {class_weights}\")\n",
        "\n",
        "        class WeightedTrainer(Trainer):\n",
        "            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "                labels = inputs.get(\"labels\").to(device)\n",
        "                outputs = model(**{k: v.to(device) for k, v in inputs.items()})\n",
        "                logits = outputs.get(\"logits\")\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "                loss = loss_fct(logits, labels)\n",
        "                return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=5,\n",
        "            learning_rate=3e-5,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            greater_is_better=True,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            logging_dir=\"./logs\",\n",
        "            logging_steps=10,\n",
        "        )\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            logits, labels = eval_pred\n",
        "            predictions = np.argmax(logits, axis=-1)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\", zero_division=1)\n",
        "            kappa = cohen_kappa_score(labels, predictions)\n",
        "            logger.info(f\"Eval predicted label distribution: {pd.Series(predictions).value_counts().to_dict()}\")\n",
        "            return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"kappa\": kappa}\n",
        "\n",
        "        trainer = WeightedTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset[\"train\"],\n",
        "            eval_dataset=tokenized_dataset[\"test\"],\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting fine-tuning...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Log training and validation loss\n",
        "        logger.info(f\"Training metrics history: {trainer.state.log_history}\")\n",
        "\n",
        "\n",
        "        # Save model\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        logger.info(f\"Model saved to {output_dir}\")\n",
        "        return output_dir\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in fine-tuning: {e}\")\n",
        "        return \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "snM8wXpPPbW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Summarization\n",
        "def summarize_text(text):\n",
        "    \"\"\"Generate summary using T5, handling short inputs.\"\"\"\n",
        "    try:\n",
        "        if len(text.strip()) < 10:\n",
        "            logger.warning(f\"Text too short for summarization: {text[:50]}...\")\n",
        "            return text[:50]\n",
        "\n",
        "        summarizer = pipeline(\n",
        "            \"summarization\",\n",
        "            model=\"t5-base\",\n",
        "            tokenizer=AutoTokenizer.from_pretrained(\"t5-base\", max_length=512, truncation=True),\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        input_length = len(text.split())\n",
        "        output_max_length = max(10, min(50, input_length // 2))\n",
        "        summary = summarizer(text[:512], max_length=output_max_length, min_length=5, do_sample=False)\n",
        "        return summary[0][\"summary_text\"]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in summarization: {e}\")\n",
        "        return text[:50]"
      ],
      "metadata": {
        "id": "zgS0L84VS27l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Aggregate Sentiment\n",
        "def aggregate_sentiment(sentiment_results):\n",
        "    \"\"\"Aggregate sentiment scores.\"\"\"\n",
        "    try:\n",
        "        sentiments = [r[\"sentiment\"] for r in sentiment_results]\n",
        "        scores = [r[\"score\"] for r in sentiment_results]\n",
        "        sentiment_counts = pd.Series(sentiments).value_counts()\n",
        "        majority_sentiment = sentiment_counts.idxmax() if not sentiment_counts.empty else \"negative\"\n",
        "        weighted_score = sum(scores) / len(scores) if scores else 0\n",
        "        return {\"majority_sentiment\": majority_sentiment, \"confidence\": weighted_score}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error aggregating sentiment: {e}\")\n",
        "        return {\"majority_sentiment\": \"negative\", \"confidence\": 0}"
      ],
      "metadata": {
        "id": "Q0B_nZINTCEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluation Metrics\n",
        "def evaluate_model(true_labels, predicted_labels):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    try:\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=\"weighted\", zero_division=1)\n",
        "        kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"kappa\": kappa}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluating model: {e}\")\n",
        "        return {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"kappa\": 0}"
      ],
      "metadata": {
        "id": "URlUYn4VTJEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Predict Sentiments with Fine-Tuned Model\n",
        "def predict_sentiment_djia(\n",
        "    file_path=None,\n",
        "    model_path=\"./distilbert-finetuned-djia\",\n",
        "    output_path=\"DJIA_Predicted_Sentiments.csv\",\n",
        "    batch_size=16,\n",
        "    max_rows=None\n",
        "):\n",
        "    \"\"\"Predict sentiment for Combined_News_DJIA dataset using the fine-tuned model.\"\"\"\n",
        "    try:\n",
        "        if file_path:\n",
        "            df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
        "        else:\n",
        "            url = \"https://raw.githubusercontent.com/niharikabalachandra/Stock-Market-Prediction-Using-Natural-Language-Processing/master/Combined_News_DJIA.csv\"\n",
        "            df = pd.read_csv(url, encoding='utf-8', low_memory=False)\n",
        "\n",
        "        # Decode byte strings in Top1 to Top25 columns\n",
        "        for col in [f'Top{i}' for i in range(1, 26)]:\n",
        "            df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
        "\n",
        "        df['text'] = df[[f'Top{i}' for i in range(1, 26)]].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "        df = df.dropna(subset=['text'])\n",
        "        df['text'] = df['text'].astype(str)\n",
        "\n",
        "        if max_rows is not None:\n",
        "            df = df.sample(n=min(max_rows, len(df)), random_state=42)\n",
        "            logger.info(f\"Subsampled to {len(df)} rows\")\n",
        "\n",
        "        if not os.path.exists(os.path.join(model_path, \"config.json\")):\n",
        "            logger.error(f\"Fine-tuned model not found at {model_path}. Please run fine-tuning first.\")\n",
        "            return None\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        logger.info(f\"Using device: {device} for prediction\")\n",
        "        sentiment_pipeline = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=model,\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model_path, max_length=512, truncation=True),\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "        scores = []\n",
        "        label_map = {\n",
        "            \"LABEL_0\": \"negative\",\n",
        "            \"LABEL_1\": \"positive\",\n",
        "            \"negative\": \"negative\",\n",
        "            \"positive\": \"positive\"\n",
        "        }\n",
        "\n",
        "        texts = df['text'].tolist()\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting sentiments\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_texts = [text[:512] for text in batch_texts]\n",
        "            results = sentiment_pipeline(batch_texts, max_length=512, truncation=True)\n",
        "\n",
        "            for result in results:\n",
        "                label = result[\"label\"]\n",
        "                predicted_label = label_map.get(label, \"negative\")\n",
        "                predictions.append(predicted_label)\n",
        "                scores.append(result[\"score\"])\n",
        "\n",
        "        df['predicted_sentiment'] = predictions\n",
        "        df['sentiment_score'] = scores\n",
        "\n",
        "        df.to_csv(output_path, index=False)\n",
        "        logger.info(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "        logger.info(\"Sample predictions:\")\n",
        "        logger.info(df[['text', 'predicted_sentiment', 'sentiment_score']].head().to_string())\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error predicting sentiments: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ffbCCSb3Z3f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Evaluation Metrics\n",
        "def evaluate_model(true_labels, predicted_labels):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    try:\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=\"weighted\", zero_division=1)\n",
        "        kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"kappa\": kappa}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluating model: {e}\")\n",
        "        return {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"kappa\": 0}"
      ],
      "metadata": {
        "id": "eKfMbtgBpzKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "def financial_insight_pipeline(texts, model_path=\"./distilbert-finetuned-djia\"):\n",
        "    \"\"\"Complete pipeline with configurable sentiment model.\"\"\"\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        sentences = preprocess_text(text)\n",
        "        entities = extract_entities(sentences)\n",
        "        relevance_results = classify_relevance(sentences)\n",
        "        relevant_sentences = [r[\"sentence\"] for r in relevance_results if r[\"is_relevant\"]]\n",
        "        if not relevant_sentences:\n",
        "            relevant_sentences = sentences[:1] or [text[:512]]\n",
        "            logger.warning(f\"No relevant sentences for text: {text[:50]}... Using first sentence.\")\n",
        "        sentiment_results = analyze_sentiment(relevant_sentences, model_path=model_path)\n",
        "        article_summary = summarize_text(\" \".join(sentences)) if sentences else text[:50]\n",
        "        aggregated_sentiment = aggregate_sentiment(sentiment_results)\n",
        "        results.append({\n",
        "            \"original_text\": text,\n",
        "            \"preprocessed_sentences\": sentences,\n",
        "            \"entities\": entities,\n",
        "            \"relevance\": relevance_results,\n",
        "            \"sentiments\": sentiment_results,\n",
        "            \"summary\": article_summary,\n",
        "            \"aggregated_sentiment\": aggregated_sentiment\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "dUTMhF4ATREp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load real dataset for testing\n",
        "def load_real_dataset(max_rows=700):  # Increased to 200 for better evaluation\n",
        "    \"\"\"Load Combined_News_DJIA dataset for testing.\"\"\"\n",
        "    try:\n",
        "        url = \"https://raw.githubusercontent.com/niharikabalachandra/Stock-Market-Prediction-Using-Natural-Language-Processing/master/Combined_News_DJIA.csv\"\n",
        "        df = pd.read_csv(url)\n",
        "        # Decode byte strings in Top1 to Top25 columns\n",
        "        for col in [f'Top{i}' for i in range(1, 26)]:\n",
        "            df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
        "        df['combined_news'] = df[[f'Top{i}' for i in range(1, 26)]].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "        df = df.head(max_rows)\n",
        "        return df['combined_news'].tolist(), df['Label'].tolist() # Return numeric labels (0,1)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading real dataset: {e}\")\n",
        "        return [\"Sample text for testing.\"] * max_rows, [0] * max_rows\n"
      ],
      "metadata": {
        "id": "7EPhqxxvTjR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation(pipeline_results, true_labels):\n",
        "    \"\"\"Evaluate sentiment classification.\"\"\"\n",
        "    try:\n",
        "      # Convert predicted string labels to numeric (negative=0, positive=1)\n",
        "        label_map = {\"negative\": 0, \"positive\": 1}\n",
        "        predicted_labels = [label_map[r[\"aggregated_sentiment\"][\"majority_sentiment\"]] for r in pipeline_results]\n",
        "\n",
        "        logger.info(f\"Number of predicted labels: {len(predicted_labels)}\")\n",
        "        logger.info(f\"Number of true labels: {len(true_labels)}\")\n",
        "        logger.info(f\"Sample predicted labels: {predicted_labels[:5]}\")\n",
        "        logger.info(f\"Sample true labels: {true_labels[:5]}\")\n",
        "        logger.info(f\"Predicted label distribution: {pd.Series(predicted_labels).value_counts().to_dict()}\")\n",
        "        logger.info(f\"True label distribution: {pd.Series(true_labels).value_counts().to_dict()}\")\n",
        "\n",
        "        if len(true_labels) != len(predicted_labels):\n",
        "            logger.error(f\"Mismatch in number of true and predicted labels: {len(true_labels)} vs {len(predicted_labels)}\")\n",
        "            min_length = min(len(true_labels), len(predicted_labels))\n",
        "            true_labels = true_labels[:min_length]\n",
        "            predicted_labels = predicted_labels[:min_length]\n",
        "            logger.warning(f\"Truncated to {min_length} samples for evaluation\")\n",
        "\n",
        "        metrics = evaluate_model(true_labels, predicted_labels)\n",
        "        logger.info(\"Evaluation Metrics:\")\n",
        "        logger.info(f\"Precision: {metrics['precision']:.3f}\")\n",
        "        logger.info(f\"Recall: {metrics['recall']:.3f}\")\n",
        "        logger.info(f\"F1-Score: {metrics['f1']:.3f}\")\n",
        "        logger.info(f\"Cohen's Kappa: {metrics['kappa']:.3f}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in evaluation: {e}\")"
      ],
      "metadata": {
        "id": "2kCq3sf8a1v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Running Financial Insight Extraction Pipeline with DistilBERT Fine-Tuning on DJIA...\")\n",
        "\n",
        "    # Fine-tune DistilBERT (test with 1000 rows, set max_rows=None for full dataset)\n",
        "    fine_tuned_model_path = fine_tune_distilbert(max_rows=None)\n",
        "\n",
        "    # Predict sentiments on DJIA dataset\n",
        "    predict_sentiment_djia(model_path=fine_tuned_model_path, max_rows=None)\n",
        "\n",
        "    # Run full pipeline on Combined_News_DJIA for testing\n",
        "    texts, true_labels = load_real_dataset(max_rows=200)  # Increased to 200\n",
        "    pipeline_results = financial_insight_pipeline(texts, model_path=fine_tuned_model_path)\n",
        "\n",
        "    run_evaluation(pipeline_results, true_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c1845f30d2c94cfe9d82a82782e2dc03",
            "26ebc46763534e81b399519c93065854",
            "9306fd756cc74c3a8b73fda7e327ff3c",
            "b970cd4541d24a1cbeab363feb62fcdb",
            "6e8211a0cb744952b629ac3299457065",
            "165012fdec674f83abea1c894774e4e9",
            "ae184dc765e14eb099c77042f40ee903",
            "dd2e9629a0ff46b28a7c4d3c730a941b",
            "0686972be64d411fbfc259233b6336ea",
            "b5c64ae553e3478ba7ff0efaba113343",
            "748710287cbc476bb3da755ee2e3784a",
            "acfdf4593abf47099b13112ee489fead",
            "aa0134107f9c4f4f8b4a071176ac74cb",
            "8543cb7f2b29435aa109999751fad8e1",
            "3a60d7dfcec649bfa29f089ea15c71a9",
            "cbf1ebd83ad64618bb4f7e43a211e0d6",
            "df98b0ba42fb46bc85cf79a2204047a6",
            "355c67fabc7340598a7fc238f27629bb",
            "8512ef9f4c824ff6960ba84b95106e13",
            "3637886fa42641ac80cc217582549d68",
            "092d87797cea41349669c9b01175b8a1",
            "9bc511535fe647bbb935f628a5dc26c7",
            "455c99bee4264bac965f2b34c8707f8a",
            "0aabceb9754f4df099327d5cc78e99e8",
            "e219cba029384764b4d287b152b5b096",
            "abab3999a0944ed5bf3f070f7fefa59f",
            "804a04cc3ac449f383a81701cda2e6c1",
            "c07b337742904f78bea21f2d8d02f59b",
            "8509656dc1e24f248c163a0825e19479",
            "773d1e3316cb4216b11a04fc3333fc96",
            "0f06997dbd9d4413a813a4fdc089d56e",
            "903c468d3d4f4742b3fb35d32587c0e8",
            "5708ae49bf284d149904b24ca32f2adb"
          ]
        },
        "collapsed": true,
        "id": "GUAoK1VRXWUQ",
        "outputId": "2458991d-8c28-4050-9e0e-6ff23e52a08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1989 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1845f30d2c94cfe9d82a82782e2dc03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1591 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acfdf4593abf47099b13112ee489fead"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/398 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "455c99bee4264bac965f2b34c8707f8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 02:39, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>0.696594</td>\n",
              "      <td>0.751237</td>\n",
              "      <td>0.464824</td>\n",
              "      <td>0.295000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.686700</td>\n",
              "      <td>0.697298</td>\n",
              "      <td>0.512837</td>\n",
              "      <td>0.530151</td>\n",
              "      <td>0.463586</td>\n",
              "      <td>0.011738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.564200</td>\n",
              "      <td>0.744230</td>\n",
              "      <td>0.514177</td>\n",
              "      <td>0.510050</td>\n",
              "      <td>0.510406</td>\n",
              "      <td>0.023208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.353800</td>\n",
              "      <td>0.984051</td>\n",
              "      <td>0.528056</td>\n",
              "      <td>0.520101</td>\n",
              "      <td>0.518484</td>\n",
              "      <td>0.049276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.147800</td>\n",
              "      <td>1.116583</td>\n",
              "      <td>0.540699</td>\n",
              "      <td>0.540201</td>\n",
              "      <td>0.540420</td>\n",
              "      <td>0.076812</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Predicting sentiments: 100%|██████████| 125/125 [00:11<00:00, 10.55it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Why wont America and Nato help us? If they wont ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Remember that adorable 9-year-old who sang at th... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Mom of missing gay man: Too bad he's not a 21-ye... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'In an Afghan prison, the majority of female pris... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Man arrested and locked up for five hours after ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"British resident held in Guantanamo Bay wins leg... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Syria says its ready to put a Russian missile sy... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"N Korea's Kim died in 2003; replaced by lookalik... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'North Korea halts denuclearisation after US fail... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Military help for Georgia is a 'declaration of w... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=19) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Russian Prime Minister Vladimir Putin has accuse... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Poland Legalization of Marijuana - two majour st... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'In Jordan, the honor killings that are not: Wome... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'A French judge has ordered two branches of Scien... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'I love the world: Thai Prime Minister Samak Sund... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Dutch to ban burkas when picking children up fro... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'American intelligence confirms that the latest m... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"OPEC finished??  Saudi Arabia says 'LATER!'\" b'P... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Brits take a stand - ban short-selling of financ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=19) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'$US 13 billion of the money allocated for recons... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Iran leader says 'American empire' near collapse... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Russian woman caught red-handed drinking orange ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The Power Of Nightmares - BBC (Part 1)' b\"Japan ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Gorbachev forms new Russian party opposing Putin... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Mexican Farm Leader Dies After Lighting Himself ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"US strikes 'kill 20 in Pakistan'\" b'A script wri... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Iceland. Sept 5/08 : UN's Best Country to Live '... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Modern slavery in Dubai' b'Brazil and Argentina,... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The Bush administration this month is quietly cu... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'EU Bans the Incandescent Light Bulb' b'AFP: Paul... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Russian Lawyer Who Defended Journalists Is Poiso... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Iceland has food stocks for about 3 to 5 weeks' ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The Other Man On The Podium' b'Dutch non-profit ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'US falls to 119th out of around 160 countries ra... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'EU formally renews ties with Cuba ' b'Terrorism ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'BBC: Rampant Evidence That Georgia Committed War... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Finnish e-voting system loses 2% of votes, has n... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"I Know We've Got This Election in US To Worry, B... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'British police ask for camera license!' b'Behead... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=13) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Every MP to receive a copy of Orwell\\'s \"Ninetee... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'London: Smokers to be banned from fostering chil... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'\"People \\'can\\'t wait for ID cards\\'\" - WTF?  Wh... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'IAEA: Documents linking Iran to nuclear weapons ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Sudan's President has announced an immediate cea... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'\"Now for being stupid and believing in witchcraf... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Russia \"Ready to buy Iceland for good money\".' b... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Putin: \"I am going to hang Saakashvili by the ba... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Haitians are so desperate for food that many mot... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'India sinks Somali pirate ship' b'New prostituti... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Denmark considers outlawing male circumcision on... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Swiss Army bans vegetarians from fighting ' b'Ko... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'London Police boycott tasers \"because of their p... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'RIP Brenden Foster, the 11-year-old whose last w... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Live updates on Mumbai Blasts' b'Johan Franzen s... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Mumbai photographer: I wish I'd had a gun, not a... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The U.K. will make it an offence, punishable by ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Brazil Announces Plan to Slow Amazon Deforestati... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'\"I fell in love with a female assassin...\"' b'Th... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Canadian Conservatives Suspend Parliament to Avo... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'False Flag?  One of the men arrested for illegal... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Canada: Wal-Mart tries to prevent employee union... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Escapee Tells of Horrors in North Korean Prison ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The unrest that has gripped Greece is spilling o... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"President Mugabe, you are under arrest on charge... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Turks apologize for Armenian massacres' b'Greek ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Chinese ship uses Molotov cocktails to fight off... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Scientist repeat 1962 Milgram test: More than 80... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Man threatened with arrest under the Australian ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Hell on Earth, this is a city of ShipBreakers. '... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'120 reportedly killed in Israeli strikes on Gaza... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Today Israel takes down an entire apt building o... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Greece: Union activist loses her eye in acid att... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Former Army Employee Pleads Guilty to Acting as ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Australia refuses Bush administration request to... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Palestine Girl stands up to Israel Soldier ' b'I... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Anyone know if this is true?' b\"British Gov't wa... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'You are being lied to about pirates' b'Rockets h... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Vatican: Gaza is a giant concentration camp' b'I... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"The video the Israeli Army didn't want you to se... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Greece hinders US arms delivery to Israel' b\"Can... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"UN headquarters in Gaza hit by Israeli 'white ph... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Vatican reveals list of worst sins. Desecrating ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Top Israeli officers warned against traveling in... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Huh? The NYTimes publishes an Op-Ed by Libyan le... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'UN human rights official Richard Falk (who is Je... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'David Attenborough reveals creationist hate mail... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Dear Reddit, please give this some attention: Co... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'A protester has thrown a shoe at Wen Jiabao duri... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Israel admits to killing peace activist Doctor's... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'India shocked as moral police beat girls out of ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Why Canada may be the financial capital of the w... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Fires in Australia from The Big Picture' b'Amnes... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Saudi judge sentences pregnant gang-rape victim ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'A teenage thief picked on the wrong victim when ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Are you a bad enough dude to have the Russian go... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Mexico just discovered oil \"3.8 times its presen... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'We are all extremists now - \"The government is c... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Graph of historic financial collapses puts curre... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Legalize It: Ammiano to Introduce Legislation Mo... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Man survives 10 days in ski mountain, spouse die... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The United States has decided to boycott an upco... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Last night an asteroid (discovered only days ago... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"The worst 'good luck' card ever? \" b'Iranian pol... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Bigoted parents don't want to see disabled peopl... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Stop Bill C-15: Say No To Mandatory Jail Terms F... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The Economist: \"... the war on drugs has been a ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Kim Jong-Il wins re-election with 99.9% of the v... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Iraqi jailed for 3 years for Bush shoe attack (c... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Israel's 22-day offensive in the Hamas-ruled Gaz... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"What will Americans do when it's not an Arab wom... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Hello Wikileaks?  Huge UK Bank gags newspaper fr... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Australian Government adds Wikileaks to banned w... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Scenes from the recession' b\"China's last eunuch... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'\"In a crowded civilian city, there are all sorts... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'[pic] Israeli tshirt: kill pregnant women, that ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Doesnt it seem ironic that in a country with tho... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Mexico's drug war [pics]\" b'An illegal settlemen... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Defamation of \"Religion\" is now a Human Rights V... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Israeli lie machine goes into overdrive -- Israe... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Western internet censorship: The beginning of th... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"If we don't kill them they'll die; the morbidly ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'I can see PALIN from my REDDIT!' b'Police car tr... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Newspaper in Mideastern nation photoshops women ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Dubai: How Not to Build a City' b'Scientology be... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The dark side of Dubai' b'Hundreds of photograph... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Mexican President Calderon: \"It is impossible to... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Anti-government protesters in Tibet sentenced to... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The film-maker Sir David Attenborough becomes a ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Overfishing to wipe out bluefin tuna: WWF' b'In ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Monsanto Uprooted: Germany Bans Cultivation of G... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'London police are now deleting tourists\\' photos... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'A poll of 60,000 Swedes showed that 89 per cent ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Israel decides to build a \"Museum of Tolerance\" ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Redditors awarded for being homo heroes' b'The P... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Israeli FM: \"Believe me, America accepts all our... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'I think more people need to understand what wate... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"Muslims dont care about Swine Flu being called S... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'While everybody its talking about the flu, Mexic... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'BREAKING NEWS: Attack on Dutch Royal family, car... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Saudi Arabia to consider banning marriage of gir... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Brazilian cattle ranching company wants permissi... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Eric Arthur Blair  aka George Orwell must be pro... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Afghanistan passes law making it \"illegal for a ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The PirateBay launches a D-DO$ attack. Wonderful... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Swedish Bank to Freeze Accounts of The Pirate Ba... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'\"I was murdered by president Alvaro Colom\"' b'\"E... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b' Unsung hero : The only reason we know anything ... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The leader of the Tamil Tiger rebels, Velupillai... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b\"'Endemic' rape and abuse of Irish children in Ca... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'The Nigerian military has been accused of killin... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'Tiananmen anniversary unimportant to Chinese you... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "WARNING:__main__:No relevant sentences for text: b'European Parliament in for a shock from The Pira... Using first sentence.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned FinBERT model\n",
        "def load_fine_tuned_model(model_path=\"./distilbert-finetuned-djia\"):\n",
        "    \"\"\"Load the fine-tuned FinBERT model for sentiment prediction.\"\"\"\n",
        "    try:\n",
        "        sentiment_pipeline = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=model_path,\n",
        "            tokenizer=model_path,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        logger.info(f\"Fine-tuned model loaded from {model_path}\")\n",
        "        return sentiment_pipeline\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading fine-tuned model: {e}\")\n",
        "        logger.warning(\"Falling back to pre-trained ProsusAI/finbert\")\n",
        "        return pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=\"ProsusAI/finbert\",\n",
        "            tokenizer=\"ProsusAI/finbert\",\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "# Compute evaluation metrics\n",
        "def compute_metrics(true_labels, predicted_labels):\n",
        "    \"\"\"Compute precision, recall, F1-score, accuracy, and Cohen's Kappa.\"\"\"\n",
        "    try:\n",
        "        # Log unique labels for debugging\n",
        "        logger.info(f\"Unique true labels: {set(true_labels)}\")\n",
        "        logger.info(f\"Unique predicted labels: {set(predicted_labels)}\")\n",
        "\n",
        "        # Ensure labels are strings\n",
        "        true_labels = [str(label) for label in true_labels]\n",
        "        predicted_labels = [str(label) for label in predicted_labels]\n",
        "\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=\"weighted\")\n",
        "        kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"kappa\": kappa,\n",
        "\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error computing metrics: {e}\")\n",
        "        return {\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"f1\": 0.0,\n",
        "            \"kappa\": 0.0,\n",
        "\n",
        "        }\n",
        "\n",
        "# Evaluate using Combined_News_DJIA test split\n",
        "def evaluate_djia(\n",
        "    file_path=None,\n",
        "    model_path=\"./distilbert-finetuned-djia\",\n",
        "    test_size=0.2,\n",
        "    max_rows=None,\n",
        "    binary=True\n",
        "):\n",
        "    \"\"\"Evaluate the fine-tuned FinBERT model using a test split from Combined_News_DJIA.\"\"\"\n",
        "    try:\n",
        "        # Load the CSV\n",
        "        if file_path:\n",
        "            df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
        "        else:\n",
        "            url = \"https://raw.githubusercontent.com/niharikabalachandra/Stock-Market-Prediction-Using-Natural-Language-Processing/master/Combined_News_DJIA.csv\"\n",
        "            df = pd.read_csv(url, encoding='utf-8', low_memory=False)\n",
        "\n",
        "        logger.info(f\"DJIA CSV loaded: {df.shape}, Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        # Combine Top1 to Top25 into a single text column\n",
        "        df['text'] = df[['Top' + str(i) for i in range(1, 26)]].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "        df = df.dropna(subset=['text', 'Label'])\n",
        "        df['text'] = df['text'].astype(str)\n",
        "        df['true_sentiment'] = df['Label'].map({0: \"negative\", 1: \"positive\"})\n",
        "\n",
        "        # Subsample for testing (optional)\n",
        "        if max_rows is not None:\n",
        "            df = df.sample(n=min(max_rows, len(df)), random_state=42)\n",
        "            logger.info(f\"Subsampled to {len(df)} rows\")\n",
        "\n",
        "        # Create Dataset and convert true_sentiment to ClassLabel\n",
        "        dataset = Dataset.from_pandas(df[['text', 'true_sentiment']])\n",
        "        dataset = dataset.cast_column('true_sentiment', ClassLabel(names=[\"negative\", \"positive\"]))\n",
        "\n",
        "        # Split with stratification\n",
        "        dataset = dataset.train_test_split(test_size=test_size, seed=42, stratify_by_column='true_sentiment')\n",
        "        df_test = dataset['test'].to_pandas()\n",
        "\n",
        "        # Convert true_sentiment back to string to avoid numeric indices\n",
        "        df_test['true_sentiment'] = df_test['true_sentiment'].map({0: \"negative\", 1: \"positive\"})\n",
        "\n",
        "        # Predict sentiments\n",
        "        sentiment_pipeline = load_fine_tuned_model(model_path)\n",
        "        predictions = []\n",
        "        scores = []\n",
        "        if binary:\n",
        "            label_map = {\n",
        "                \"LABEL_0\": \"negative\",\n",
        "                \"LABEL_1\": \"positive\",\n",
        "                \"negative\": \"negative\",\n",
        "                \"positive\": \"positive\",\n",
        "                \"neutral\": \"negative\"  # Map neutral to negative for binary evaluation\n",
        "            }\n",
        "        else:\n",
        "            label_map = {\n",
        "                \"LABEL_0\": \"negative\",\n",
        "                \"LABEL_1\": \"neutral\",\n",
        "                \"LABEL_2\": \"positive\",\n",
        "                \"negative\": \"negative\",\n",
        "                \"neutral\": \"neutral\",\n",
        "                \"positive\": \"positive\"\n",
        "            }\n",
        "\n",
        "        # Batch processing for efficiency\n",
        "        batch_size = 16\n",
        "        texts = df_test['text'].tolist()\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting sentiments\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_texts = [text[:512] for text in batch_texts]\n",
        "            results = sentiment_pipeline(batch_texts)\n",
        "            for result in results:\n",
        "                label = result[\"label\"]\n",
        "                predicted_label = label_map.get(label, \"negative\")\n",
        "                predictions.append(predicted_label)\n",
        "                scores.append(result[\"score\"])\n",
        "\n",
        "        df_test['predicted_sentiment'] = predictions\n",
        "        df_test['sentiment_score'] = scores\n",
        "\n",
        "        # Compute metrics\n",
        "        true_labels = df_test['true_sentiment'].tolist()\n",
        "        predicted_labels = df_test['predicted_sentiment'].tolist()\n",
        "        metrics = compute_metrics(true_labels, predicted_labels)\n",
        "\n",
        "        # Log metrics\n",
        "        logger.info(\"Evaluation Metrics (Combined_News_DJIA):\")\n",
        "        logger.info(f\"Precision: {metrics['precision']:.3f}\")\n",
        "        logger.info(f\"Recall: {metrics['recall']:.3f}\")\n",
        "        logger.info(f\"F1-Score: {metrics['f1']:.3f}\")\n",
        "        logger.info(f\"Cohen's Kappa: {metrics['kappa']:.3f}\")\n",
        "\n",
        "\n",
        "        # Log sample comparisons\n",
        "        logger.info(\"Sample comparisons:\")\n",
        "        logger.info(df_test[['text', 'true_sentiment', 'predicted_sentiment', 'sentiment_score']].head().to_string())\n",
        "\n",
        "        # Save results\n",
        "        df_test.to_csv(\"djia_evaluation_results.csv\", index=False)\n",
        "        logger.info(\"Results saved to djia_evaluation_results.csv\")\n",
        "\n",
        "        return metrics, df_test\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluating DJIA dataset: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run the evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Primary: Evaluate on Combined_News_DJIA test split\n",
        "    logger.info(\"Evaluating on Combined_News_DJIA...\")\n",
        "    metrics_djia, df_djia = evaluate_djia(max_rows=1000, binary=True)  # Limit for testing\n",
        "    if metrics_djia:\n",
        "        logger.info(\"DJIA evaluation completed successfully\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AXUxThslx1Pu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "b5dedae3b30a42e285b9a8ce86b02b20",
            "4524da5630cc4aba98baa46bb38d08f2",
            "4c74744689c94fcb99e879b434b0ce17",
            "0995f47960d24bdeba16a6dcebaf7123",
            "a0e0e07be18045e9ac94aa7634f3bf8b",
            "7ee81ffd3a1f471f93a9cc764162e2e8",
            "8586160623a54f51becb0ae4d2447938",
            "4c3b31dee44c4f6189cafb4d14f0f5ae",
            "5df40858fa5c441596c73a67847c1963",
            "9334d6169c1e45f7afcd8e5d2134beb1",
            "c0a77c9b16e64da5aa7052c813e87690"
          ]
        },
        "outputId": "7bc4e556-50a8-458e-84c6-58aeac3db582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5dedae3b30a42e285b9a8ce86b02b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Predicting sentiments: 100%|██████████| 13/13 [00:01<00:00, 10.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_djia_predicted= pd.read_csv('/content/djia_evaluation_results.csv')\n",
        "df_djia_predicted.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CKZYQioiRU3H",
        "outputId": "4ea3a7b6-6d44-4b01-93a6-6f7f50ae0398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text true_sentiment  \\\n",
              "0  b'Pirate Party Wins and Enters The European Pa...       positive   \n",
              "1  Iranians respond to Israeli Facebook initiativ...       positive   \n",
              "2  German footballer Mesut Ozil donated his 300,0...       negative   \n",
              "3  Japan Prime Minister will give up his salary u...       positive   \n",
              "4  Mexico's Drug War: 50,000 Dead in 6 Years As a...       negative   \n",
              "\n",
              "   __index_level_0__ predicted_sentiment  sentiment_score  \n",
              "0                208            negative         0.950247  \n",
              "1                909            negative         0.503802  \n",
              "2               1494            negative         0.877722  \n",
              "3                693            positive         0.627974  \n",
              "4                952            positive         0.957462  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f283fbaa-758d-44bf-a82f-45e642582cdd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>true_sentiment</th>\n",
              "      <th>__index_level_0__</th>\n",
              "      <th>predicted_sentiment</th>\n",
              "      <th>sentiment_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'Pirate Party Wins and Enters The European Pa...</td>\n",
              "      <td>positive</td>\n",
              "      <td>208</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.950247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iranians respond to Israeli Facebook initiativ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>909</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.503802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>German footballer Mesut Ozil donated his 300,0...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1494</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.877722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Japan Prime Minister will give up his salary u...</td>\n",
              "      <td>positive</td>\n",
              "      <td>693</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.627974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mexico's Drug War: 50,000 Dead in 6 Years As a...</td>\n",
              "      <td>negative</td>\n",
              "      <td>952</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.957462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f283fbaa-758d-44bf-a82f-45e642582cdd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f283fbaa-758d-44bf-a82f-45e642582cdd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f283fbaa-758d-44bf-a82f-45e642582cdd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-efb966c1-410a-4a57-90b6-2a8afa3e84ac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-efb966c1-410a-4a57-90b6-2a8afa3e84ac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-efb966c1-410a-4a57-90b6-2a8afa3e84ac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_djia_predicted",
              "summary": "{\n  \"name\": \"df_djia_predicted\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"UN: Online Privacy Now Considered a Human Right Uganda president refuses to approve anti-gay Bill The Economist talks about \\\"The coming storm\\\" - 47% of today's jobs could be automated within 20 years \\\"No government is prepared for it.\\\" Suggests a radical rethinking of education and redistributive taxes towards lower-wage workers 84-year-old Canadian died in handcuffs while in U.K. immigration lockup: watchdog NSA collects millions of text messages daily in 'untargeted' global sweep Fervently Anti-Gay Uganda Among World's Top Gay Porn Consumers Behind Fervently Anti-Gay Pakistan Growing Number Of People Agree That Ed Snowden Is A Whistleblower The self-proclaimed mastermind of the September 11 attacks, Khalid Sheikh Mohammed, has released a manifesto claiming that the Quran forbids the use of violence to spread Islam Neurosurgeons from Poland transplanted nerve cells taken from the nose of man that had his spinal cord injured. He can move his legs now. (Google translate) Japan's last WWII straggler dies at 91 - Hiroo Onoda, the last Japanese imperial soldier to emerge from hiding and surrender 29 years after the end of World War II, has died. Ugandan president refuses to approve law jailing gay people for life Putin's Message to Gays in Sochi: Leave Children in Peace - Gay people should feel comfortable at the Sochi Olympics but leave children in peace, Russias President Vladimir Putin said Friday. Spain becomes first country to rely on wind as top energy source Mexican vigilante groups refuse to be disarmed by the military Activists in Wuhan block truck carrying 2,800 cats headed for Slaughter Italian MP puts on blackface in anti-immigration tirade, \\\"saying that all white Italians should do the same so as to receive benefits, free housing and preferential treatment African migrants do\\\" Buddhist Mob Group Kills Dozens of Muslims in Myanmar. Vladimir Putin: \\\"We do not have a ban on non-traditional sexual relationships. We have a ban on the propaganda of homosexuality and paedophilia. I want to underline this. Propaganda among children. These are absolutely different things  a ban on something or a ban on the propaganda of that thing.\\\" Young Greek convicted 10 months with parole for insulting religion, by facebook page with the Greek offshoot of the Flying Spaghetti Monster. A 79-year old feminist, peace activist, film-maker and member of Aosdna has been jailed for three months in Limerick Prison in relation to protests over US military use of Shannon Airport. Vomit fee proposed for Torontos taxi cabs Metals, Currency Rigging Worse Than Libor, Bafin Chief Says A blogger in Greece has been sentenced to 10 months in prison for blasphemy because he satirized a Greek Orthodox monk Jamaican Bobsled Team on verge of qualifying for Sochi Olympics, first time since 2002 Ukraine passes sweeping legislation against acts of protest\",\n          \"Govt to axe Australian Renewable Energy Agency Russia blogger bill authorized today, requires bloggers to publicize identity, all posts will be inspected by government officials and any \\\"terrorist or propagating posts\\\" are subject to fines and imprisonment. Japanese whalers have restarted operations in the north-west Pacific only weeks after the United Nation's highest court banned Japan's so-called scientific whaling program in the Antarctic Brazil Police warn visitors, 'Don't scream if robbed' Cocaine use in Britain so high it has contaminated our drinking water, report shows Over 540 million people (66.4% of electorate) just finished voting in India's month-long general election, the largest democratic exercise in the world to date The Pirate Bay to be Blocked by Australian Government IEA: Decarbonising the economy will save $71 trillion by 2050 2,000 tons of whale meat arrives in Japan from Iceland Lost Vincent van Gogh painting found in bank safe Photos of dead turtles in Chinese ship anger Filipinos Ukraine Guardsmen open fire on crowd. Nereus deep sea sub 'implodes' 10km-down David Cameron: Taxes will rise unless we can raid bank accounts 400 US mercenaries 'deployed on ground' in Ukraine military op One billion people still defecate in public despite health risks-UN France to redraw nations map to save money Ukrainian troops open artillery fire at village of Adreyevka, Donetsk region - headquarters of Donetsk region's self-defense forces Obama aims oil weapon at Putin but will he pull the trigger? Oil prices heading for major correction after Russia's attempt to use crude as a weapon to bully Western powers backfires Brazil Built The World's Second-Most Expensive Soccer Stadium In A City With No Pro Team Court orders Turkey to pay Cyprus over invasion: Europes top human rights court on Monday ordered Turkey to pay 90 million euros ($123 million) to Cyprus over the 1974 invasion of the island and its subsequent division, in one of the largest judgments in its history. Russian officials(Rogozin) attempted to smuggle lists calling on independence of Transnistria. The lists were confiscated by Moldavian officers. Mexico: A Zetas founder among 6 dead in shootout Girl who escaped Boko Haram abduction speaks publicly about ordeal Ukraine crisis: National guardsmen fire into crowd - World\",\n          \"Iraq is rushing to digitize its national library under the threat of ISIS Zimbabwe bans lion hunting after international outcry Delta bans shipment of lion, leopard, elephant, rhino, buffalo trophies Saudi ministry: 'Free expression is an abuse of religious rights' New Study from Finland: People would be happier living near a drug rehab center than living near a mosque. After having praised the friendliness of the sport, the president of a bullfighting club was gored by a bull after it jumped out of the ring and attacked him viciously. 8 Eight suitcases full of ivory seized at Zurich airport - Elephant tusks with estimated black-market value of about 265,000 were being transported from Tanzania to China via Switzerland. The tusks had been sawed into pieces to fit into the luggage. Russia makes a new claim for the Northpole Stop burning fossil fuels now: there is no CO2 'technofix', scientists warn - Researchers have demonstrated that even if a geoengineering solution to CO2 emissions could be found, it wouldnt be enough to save the oceans Airbus patents jet to fly London-New York in 1 hour Bitcoin deemed regular currency by Australian Senate Committee It is worse than Hitler, worse than AIDS, cancer or any other epidemic. It is more catastrophic than nuclear holocaust, and it must be stopped. -Creator of the list of the banned Indian porn sites Canada, a resource economy, is the only G7 country in a recession United Joins Delta Banning Big-Game Trophies After Cecil Killing Obama says no challenge greater threat to U.S. future than climate change A massive gate unearthed in Israel may have marked the entrance to a biblical city that, at its heyday, was the biggest metropolis in the region. G20 countries pay over $1,000 per citizen in fossil fuel subsidies, say IMF Worlds leading economies still paying trillions in subsidies despite pledges to phase them out, new figures show China is demanding that the Obama administration return a wealthy and politically connected businessman who fled to the United States, according to several American officials familiar with the case. Should he seek political asylum, he could become one of the most damaging defectors 7.5 year prison sentence sought for 18 Turkish journalists for reporting on alleged covert arms shipments by the Turkish government into Syria. Isis 'price list' for child slaves confirmed as genuine by UN official Zainab Bangura Women Killed Alongside Mexican Photojournalist Were Tortured &amp; Raped - 3 of the 4 women with Ruben Espinosa were assaulted before being shot in the head after an all-night party among friends in a middle-class section of Mexico City Russia calls for international cooperation to fight Islamic State Speed of glacier retreat worldwide 'historically unprecedented', says report. Researchers have recorded rapid rises in meltwater and alarming rates of glacial retreat, which are accelerating at a pace double that of a decade ago. Israeli president flooded with death threats for condemning 'Jewish terror' More than 450 civilians killed in US-led airstrikes against the Islamic State\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"true_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"__index_level_0__\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 589,\n        \"min\": 6,\n        \"max\": 1984,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          1370,\n          1448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11779710644077117,\n        \"min\": 0.5038015842437744,\n        \"max\": 0.9786002039909364,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          0.9233148097991944,\n          0.608988344669342\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the fine-tuned FinBERT model"
      ],
      "metadata": {
        "id": "ah1arNF0UkKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Primary: Evaluate on Combined_News_DJIA test split\n",
        "    logger.info(\"Evaluating on Combined_News_DJIA...\")\n",
        "    # Convert the 'Label' column to string type to match the predicted_sentiment type\n",
        "    true_labels_str = df_djia_predicted['true_sentiment'].astype(str).tolist()\n",
        "    predicted_labels_str = df_djia_predicted['predicted_sentiment'].tolist()\n",
        "\n",
        "    metrics_djia = evaluate_model(true_labels_str, predicted_labels_str)  # Corrected function call as evaluate_model only returns metrics\n",
        "    if metrics_djia:\n",
        "        logger.info(\"DJIA evaluation completed successfully\")\n",
        "        print(\"\\nCombined_News_DJIA Metrics:\")\n",
        "        print(f\"Precision: {metrics_djia['precision']:.3f}\")\n",
        "        print(f\"Recall: {metrics_djia['recall']:.3f}\")\n",
        "        print(f\"F1-Score: {metrics_djia['f1']:.3f}\")\n",
        "        print(f\"Cohen's Kappa: {metrics_djia['kappa']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpDi34_WR3UY",
        "outputId": "83c82d7a-010a-45e3-9fc4-2c840e06e6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combined_News_DJIA Metrics:\n",
            "Precision: 0.858\n",
            "Recall: 0.855\n",
            "F1-Score: 0.855\n",
            "Cohen's Kappa: 0.709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Fine-tuned FinBERT model on FinSen_US_Categorized_Timestamp.csv dataset to predict the sentiments."
      ],
      "metadata": {
        "id": "aD3QuthoUXN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiments on the FinSen dataset\n",
        "from tqdm import tqdm\n",
        "def predict_sentiment_finsen(\n",
        "    file_path=\"/content/FinSen_US_Categorized_Timestamp.csv\",\n",
        "    model_path=\"/content/distilbert-finetuned-djia\",\n",
        "    output_path=\"/content/FinSen_US_Predicted_Sentiments.csv\",\n",
        "    batch_size=16,\n",
        "    max_rows=None):\n",
        "    \"\"\"Predict sentiment for FinSen dataset using the fine-tuned FinBERT model.\"\"\"\n",
        "    try:\n",
        "        # Load the CSV\n",
        "        df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
        "        logger.info(f\"CSV loaded: {df.shape}, Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        # Verify required column\n",
        "        if 'Content' not in df.columns:\n",
        "            raise ValueError(f\"Expected 'Content' column, found {df.columns.tolist()}\")\n",
        "\n",
        "        df = df.dropna(subset=['Content'])\n",
        "        df['text'] = df['Content'].astype(str)\n",
        "\n",
        "        # Subsample for testing (optional)\n",
        "        if max_rows is not None:\n",
        "            df = df.sample(n=min(max_rows, len(df)), random_state=42)\n",
        "            logger.info(f\"Subsampled to {len(df)} rows\")\n",
        "\n",
        "        # Load the model\n",
        "        sentiment_pipeline = load_fine_tuned_model(model_path)\n",
        "\n",
        "        # Predict sentiments in batches\n",
        "        predictions = []\n",
        "        scores = []\n",
        "        label_map = {\n",
        "            \"LABEL_0\": \"negative\",\n",
        "            \"LABEL_1\": \"neutral\",\n",
        "            \"LABEL_2\": \"positive\",\n",
        "            \"negative\": \"negative\",\n",
        "            \"neutral\": \"neutral\",\n",
        "            \"positive\": \"positive\"\n",
        "        }\n",
        "\n",
        "        # Process texts in batches with progress bar\n",
        "        texts = df['text'].tolist()\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting sentiments\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_texts = [text[:512] for text in batch_texts]  # Pre-truncate\n",
        "            results = sentiment_pipeline(batch_texts)\n",
        "\n",
        "            for result in results:\n",
        "                label = result[\"label\"]\n",
        "                predicted_label = label_map.get(label, \"unknown\")\n",
        "                predictions.append(predicted_label)\n",
        "                scores.append(result[\"score\"])\n",
        "\n",
        "        df['predicted_sentiment'] = predictions\n",
        "        df['sentiment_score'] = scores\n",
        "\n",
        "        # Save the updated CSV\n",
        "        df.to_csv(output_path, index=False)\n",
        "        logger.info(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "        # Log sample predictions\n",
        "        logger.info(\"Sample predictions:\")\n",
        "        logger.info(df[['Content', 'predicted_sentiment', 'sentiment_score']].head().to_string())\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error predicting sentiments: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "SNIgP1khT15f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the prediction\n",
        "if __name__ == \"__main__\":\n",
        "    predict_sentiment_finsen()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geGmBp_OZfhI",
        "outputId": "f8f5e7de-8069-410b-bfb7-06c798bd5c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Predicting sentiments: 100%|██████████| 971/971 [01:29<00:00, 10.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_predicted = pd.read_csv(\"/content/FinSen_US_Predicted_Sentiments.csv\")\n",
        "df_predicted.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N_OwlYaW_vXJ",
        "outputId": "5d45cc7a-d9f7-4a06-cba7-4709cd3438c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Title  \\\n",
              "0               TSX Slightly Down, Books Weekly Gains   \n",
              "1                       UnitedHealth Hits 4-week High   \n",
              "2                       Cisco Systems Hits 4-week Low   \n",
              "3                              AT&T Hits All-time Low   \n",
              "4                          Microsoft Hits 4-week High   \n",
              "5                         JPMorgan Hits 16-month High   \n",
              "6            US Export Prices Fall More than Expected   \n",
              "7   Citigroup earnings above expectations at 1.37 USD   \n",
              "8               US Treasury Yields Below Recent Highs   \n",
              "9   Wells Fargo earnings above expectations at 1.2...   \n",
              "10  BlackRock earnings above expectations at 9.28 USD   \n",
              "11  UnitedHealth earnings above expectations at 6....   \n",
              "12               Dollar Languishes on Dovish Fed Bets   \n",
              "13               Bitcoin Climbs as US Inflation Slows   \n",
              "14  US Budget Deficit Widens More than Expected in...   \n",
              "15                             Visa Hits 24-week High   \n",
              "16                           Amazon Hits 43-week High   \n",
              "17       10-Year Treasury Yield Falls for 4th Session   \n",
              "18                                 DXY Approaches 100   \n",
              "19               US Core PPI Rises Less than Expected   \n",
              "\n",
              "                         Tag        Time  \\\n",
              "0               Stock Market  16/07/2023   \n",
              "1                     stocks  15/07/2023   \n",
              "2                     stocks  15/07/2023   \n",
              "3                     stocks  15/07/2023   \n",
              "4                     stocks  15/07/2023   \n",
              "5                     stocks  15/07/2023   \n",
              "6          Export Prices MoM  15/07/2023   \n",
              "7                   Earnings  15/07/2023   \n",
              "8        Government Bond 10Y  15/07/2023   \n",
              "9                   Earnings  15/07/2023   \n",
              "10                  Earnings  15/07/2023   \n",
              "11                  Earnings  15/07/2023   \n",
              "12                  Currency  15/07/2023   \n",
              "13                  Currency  15/07/2023   \n",
              "14   Government Budget Value  14/07/2023   \n",
              "15                    stocks  14/07/2023   \n",
              "16                    stocks  14/07/2023   \n",
              "17       Government Bond 10Y  14/07/2023   \n",
              "18                  Currency  14/07/2023   \n",
              "19  Core Producer Prices MoM  14/07/2023   \n",
              "\n",
              "                                              Content  \\\n",
              "0   TSX Slightly Down, Books Weekly GainsUnited St...   \n",
              "1   UnitedHealth Hits 4-week HighUnited States sto...   \n",
              "2   Cisco Systems Hits 4-week LowUnited States sto...   \n",
              "3   AT&T Hits All-time LowUnited States stocksAT&T...   \n",
              "4   Microsoft Hits 4-week HighUnited States stocks...   \n",
              "5   JPMorgan Hits 16-month HighUnited States stock...   \n",
              "6   US Export Prices Fall More than ExpectedUnited...   \n",
              "7   Citigroup earnings above expectations at 1.37 ...   \n",
              "8   US Treasury Yields Below Recent Highs United S...   \n",
              "9   Wells Fargo earnings above expectations at 1.2...   \n",
              "10  BlackRock earnings above expectations at 9.28 ...   \n",
              "11  UnitedHealth earnings above expectations at 6....   \n",
              "12  Dollar Languishes on Dovish Fed BetsUnited Sta...   \n",
              "13  Bitcoin Climbs as US Inflation SlowsUnited Sta...   \n",
              "14  US Budget Deficit Widens More than Expected in...   \n",
              "15  Visa Hits 24-week HighUnited States stocksVisa...   \n",
              "16  Amazon Hits 43-week HighUnited States stocksAm...   \n",
              "17  10-Year Treasury Yield Falls for 4th SessionUn...   \n",
              "18  DXY Approaches 100United States CurrencyThe do...   \n",
              "19  US Core PPI Rises Less than ExpectedUnited Sta...   \n",
              "\n",
              "                                                 text predicted_sentiment  \\\n",
              "0   TSX Slightly Down, Books Weekly GainsUnited St...            negative   \n",
              "1   UnitedHealth Hits 4-week HighUnited States sto...             neutral   \n",
              "2   Cisco Systems Hits 4-week LowUnited States sto...            negative   \n",
              "3   AT&T Hits All-time LowUnited States stocksAT&T...            negative   \n",
              "4   Microsoft Hits 4-week HighUnited States stocks...            negative   \n",
              "5   JPMorgan Hits 16-month HighUnited States stock...            negative   \n",
              "6   US Export Prices Fall More than ExpectedUnited...             neutral   \n",
              "7   Citigroup earnings above expectations at 1.37 ...            negative   \n",
              "8   US Treasury Yields Below Recent Highs United S...            negative   \n",
              "9   Wells Fargo earnings above expectations at 1.2...            negative   \n",
              "10  BlackRock earnings above expectations at 9.28 ...             neutral   \n",
              "11  UnitedHealth earnings above expectations at 6....             neutral   \n",
              "12  Dollar Languishes on Dovish Fed BetsUnited Sta...             neutral   \n",
              "13  Bitcoin Climbs as US Inflation SlowsUnited Sta...            negative   \n",
              "14  US Budget Deficit Widens More than Expected in...             neutral   \n",
              "15  Visa Hits 24-week HighUnited States stocksVisa...            negative   \n",
              "16  Amazon Hits 43-week HighUnited States stocksAm...            negative   \n",
              "17  10-Year Treasury Yield Falls for 4th SessionUn...             neutral   \n",
              "18  DXY Approaches 100United States CurrencyThe do...            negative   \n",
              "19  US Core PPI Rises Less than ExpectedUnited Sta...            negative   \n",
              "\n",
              "    sentiment_score  \n",
              "0          0.913185  \n",
              "1          0.715624  \n",
              "2          0.553907  \n",
              "3          0.724286  \n",
              "4          0.814781  \n",
              "5          0.729421  \n",
              "6          0.828072  \n",
              "7          0.686106  \n",
              "8          0.876244  \n",
              "9          0.607130  \n",
              "10         0.579740  \n",
              "11         0.846858  \n",
              "12         0.931086  \n",
              "13         0.645860  \n",
              "14         0.572536  \n",
              "15         0.703935  \n",
              "16         0.565901  \n",
              "17         0.545597  \n",
              "18         0.844611  \n",
              "19         0.622619  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-034f0592-ecf3-4baf-b684-74151ecb2d6e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Time</th>\n",
              "      <th>Content</th>\n",
              "      <th>text</th>\n",
              "      <th>predicted_sentiment</th>\n",
              "      <th>sentiment_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TSX Slightly Down, Books Weekly Gains</td>\n",
              "      <td>Stock Market</td>\n",
              "      <td>16/07/2023</td>\n",
              "      <td>TSX Slightly Down, Books Weekly GainsUnited St...</td>\n",
              "      <td>TSX Slightly Down, Books Weekly GainsUnited St...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.913185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UnitedHealth Hits 4-week High</td>\n",
              "      <td>stocks</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>UnitedHealth Hits 4-week HighUnited States sto...</td>\n",
              "      <td>UnitedHealth Hits 4-week HighUnited States sto...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.715624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cisco Systems Hits 4-week Low</td>\n",
              "      <td>stocks</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>Cisco Systems Hits 4-week LowUnited States sto...</td>\n",
              "      <td>Cisco Systems Hits 4-week LowUnited States sto...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.553907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AT&amp;T Hits All-time Low</td>\n",
              "      <td>stocks</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>AT&amp;T Hits All-time LowUnited States stocksAT&amp;T...</td>\n",
              "      <td>AT&amp;T Hits All-time LowUnited States stocksAT&amp;T...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.724286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Microsoft Hits 4-week High</td>\n",
              "      <td>stocks</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>Microsoft Hits 4-week HighUnited States stocks...</td>\n",
              "      <td>Microsoft Hits 4-week HighUnited States stocks...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.814781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>JPMorgan Hits 16-month High</td>\n",
              "      <td>stocks</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>JPMorgan Hits 16-month HighUnited States stock...</td>\n",
              "      <td>JPMorgan Hits 16-month HighUnited States stock...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.729421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>US Export Prices Fall More than Expected</td>\n",
              "      <td>Export Prices MoM</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>US Export Prices Fall More than ExpectedUnited...</td>\n",
              "      <td>US Export Prices Fall More than ExpectedUnited...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.828072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Citigroup earnings above expectations at 1.37 USD</td>\n",
              "      <td>Earnings</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>Citigroup earnings above expectations at 1.37 ...</td>\n",
              "      <td>Citigroup earnings above expectations at 1.37 ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.686106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>US Treasury Yields Below Recent Highs</td>\n",
              "      <td>Government Bond 10Y</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>US Treasury Yields Below Recent Highs United S...</td>\n",
              "      <td>US Treasury Yields Below Recent Highs United S...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.876244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Wells Fargo earnings above expectations at 1.2...</td>\n",
              "      <td>Earnings</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>Wells Fargo earnings above expectations at 1.2...</td>\n",
              "      <td>Wells Fargo earnings above expectations at 1.2...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.607130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>BlackRock earnings above expectations at 9.28 USD</td>\n",
              "      <td>Earnings</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>BlackRock earnings above expectations at 9.28 ...</td>\n",
              "      <td>BlackRock earnings above expectations at 9.28 ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.579740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>UnitedHealth earnings above expectations at 6....</td>\n",
              "      <td>Earnings</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>UnitedHealth earnings above expectations at 6....</td>\n",
              "      <td>UnitedHealth earnings above expectations at 6....</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.846858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Dollar Languishes on Dovish Fed Bets</td>\n",
              "      <td>Currency</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>Dollar Languishes on Dovish Fed BetsUnited Sta...</td>\n",
              "      <td>Dollar Languishes on Dovish Fed BetsUnited Sta...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.931086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Bitcoin Climbs as US Inflation Slows</td>\n",
              "      <td>Currency</td>\n",
              "      <td>15/07/2023</td>\n",
              "      <td>Bitcoin Climbs as US Inflation SlowsUnited Sta...</td>\n",
              "      <td>Bitcoin Climbs as US Inflation SlowsUnited Sta...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.645860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>US Budget Deficit Widens More than Expected in...</td>\n",
              "      <td>Government Budget Value</td>\n",
              "      <td>14/07/2023</td>\n",
              "      <td>US Budget Deficit Widens More than Expected in...</td>\n",
              "      <td>US Budget Deficit Widens More than Expected in...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.572536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Visa Hits 24-week High</td>\n",
              "      <td>stocks</td>\n",
              "      <td>14/07/2023</td>\n",
              "      <td>Visa Hits 24-week HighUnited States stocksVisa...</td>\n",
              "      <td>Visa Hits 24-week HighUnited States stocksVisa...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.703935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Amazon Hits 43-week High</td>\n",
              "      <td>stocks</td>\n",
              "      <td>14/07/2023</td>\n",
              "      <td>Amazon Hits 43-week HighUnited States stocksAm...</td>\n",
              "      <td>Amazon Hits 43-week HighUnited States stocksAm...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.565901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>10-Year Treasury Yield Falls for 4th Session</td>\n",
              "      <td>Government Bond 10Y</td>\n",
              "      <td>14/07/2023</td>\n",
              "      <td>10-Year Treasury Yield Falls for 4th SessionUn...</td>\n",
              "      <td>10-Year Treasury Yield Falls for 4th SessionUn...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.545597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>DXY Approaches 100</td>\n",
              "      <td>Currency</td>\n",
              "      <td>14/07/2023</td>\n",
              "      <td>DXY Approaches 100United States CurrencyThe do...</td>\n",
              "      <td>DXY Approaches 100United States CurrencyThe do...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.844611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>US Core PPI Rises Less than Expected</td>\n",
              "      <td>Core Producer Prices MoM</td>\n",
              "      <td>14/07/2023</td>\n",
              "      <td>US Core PPI Rises Less than ExpectedUnited Sta...</td>\n",
              "      <td>US Core PPI Rises Less than ExpectedUnited Sta...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.622619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-034f0592-ecf3-4baf-b684-74151ecb2d6e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-034f0592-ecf3-4baf-b684-74151ecb2d6e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-034f0592-ecf3-4baf-b684-74151ecb2d6e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b7989127-a9eb-4a70-a0e9-f21975755c19\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7989127-a9eb-4a70-a0e9-f21975755c19')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b7989127-a9eb-4a70-a0e9-f21975755c19 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_predicted",
              "summary": "{\n  \"name\": \"df_predicted\",\n  \"rows\": 15534,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12980,\n        \"samples\": [\n          \"US Crude Stocks Unexpectedly Rise for 2nd Week\",\n          \"US Service Sector Activity Expands the Most Since 2015\",\n          \"US Dollar Steady After Recent Weakness\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 144,\n        \"samples\": [\n          \"Producer Price Inflation MoM\",\n          \"Imports\",\n          \"Total Vehicle Sales\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 2971,\n        \"samples\": [\n          \"2/05/2013\",\n          \"16/11/2018\",\n          \"1/04/2013\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15523,\n        \"samples\": [\n          \"DXY Erases GainsUnited States\\u00a0CurrencyThe dollar index fell more than 0.5% to below 105 from an 20-year high of 105.8 hit earlier in the session as risk sentiment returned to markets after it became clear that the Fed is fully committed to fighting the surging inflation. The Federal Reserve hiked the fed funds rate by 75bps, the most since 1994 while Chair Powell signaled a similar move could come at the next meeting but he does not expect 75bps moves to be common. The sharp increase in rates comes after last week the CPI report showed inflation surged unexpectedly to a 41-year high of 8.6% in May. 2022-06-15T19:50:00\",\n          \"US Unemployment Rate Down to 5-1/2 Year LowUnited States\\u00a0Unemployment RateIn April of 2014, the jobless rate fell from 6.7 percent to 6.3 percent, the lowest rate since September of 2008, as both unemployed reentering the labor market and new entrants into the labor force fell.2014-05-02T13:59:50.763\",\n          \"US Service Sector Growth Hits 8-1/2 Year High in JulyUnited States\\u00a0Non Manufacturing PmiISM non-manufacturing index rose to 58.7 in July from 56 in June, the highest since December 2005. The number was boosted by growth in business activity, new orders and employment.2014-08-05T15:13:57.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15523,\n        \"samples\": [\n          \"DXY Erases GainsUnited States\\u00a0CurrencyThe dollar index fell more than 0.5% to below 105 from an 20-year high of 105.8 hit earlier in the session as risk sentiment returned to markets after it became clear that the Fed is fully committed to fighting the surging inflation. The Federal Reserve hiked the fed funds rate by 75bps, the most since 1994 while Chair Powell signaled a similar move could come at the next meeting but he does not expect 75bps moves to be common. The sharp increase in rates comes after last week the CPI report showed inflation surged unexpectedly to a 41-year high of 8.6% in May. 2022-06-15T19:50:00\",\n          \"US Unemployment Rate Down to 5-1/2 Year LowUnited States\\u00a0Unemployment RateIn April of 2014, the jobless rate fell from 6.7 percent to 6.3 percent, the lowest rate since September of 2008, as both unemployed reentering the labor market and new entrants into the labor force fell.2014-05-02T13:59:50.763\",\n          \"US Service Sector Growth Hits 8-1/2 Year High in JulyUnited States\\u00a0Non Manufacturing PmiISM non-manufacturing index rose to 58.7 in July from 56 in June, the highest since December 2005. The number was boosted by growth in business activity, new orders and employment.2014-08-05T15:13:57.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neutral\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12076502437004864,\n        \"min\": 0.5000364184379578,\n        \"max\": 0.9671211838722228,\n        \"num_unique_values\": 15502,\n        \"samples\": [\n          0.6373005509376526,\n          0.5452212691307068\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
